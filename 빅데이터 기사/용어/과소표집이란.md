## ✅ 과소표집(Undersampling)이란?

**과소표집**은 **데이터의 클래스 불균형을 해결하기 위해** 다수 클래스로부터 일부 데이터를 **의도적으로 줄이는 기법**이야.

예를 들어,  
- 스팸 메일: 9,000건  
- 일반 메일: 1,000건  

이렇게 **비율이 너무 차이날 때**, 스팸 메일 데이터를 1,000건 정도로 줄여서  
**"스팸 vs 일반"의 비율을 1:1로 맞추는 방식**이 과소표집이야.

---

## ✅ 과소표집의 종류

### 1. **무작위 과소표집 (Random Undersampling)**
- 다수 클래스에서 **임의로 샘플을 제거**함.
- 쉽고 빠르지만, **유용한 정보까지 손실될 수 있음**.

> 예: 스팸 메일 9,000건 중 랜덤하게 8,000건 제거

---

### 2. **클러스터 중심 과소표집 (Cluster Centroid Undersampling)**
- 다수 클래스 데이터를 클러스터링해서, **중심값(centroid)만 남김**.
- 데이터의 대표성을 살리면서도 양을 줄이는 방식.

---

### 3. **Tomek Links**
- 서로 다른 클래스 간 거리가 가까운 데이터 쌍을 찾아 제거함.
- 주로 **중첩된(애매한) 경계 데이터를 제거**해서 경계를 더 분명히 함.

---

### 4. **NearMiss (1, 2, 3)**
- **소수 클래스와의 거리 기반**으로 다수 클래스를 선별해 제거함.
  - **NearMiss-1**: 소수 클래스와 가까운 다수 클래스를 남김.
  - **NearMiss-2**: 소수 클래스와 먼 다수 클래스를 제거.
  - **NearMiss-3**: 소수 클래스 주변에서 가장 가까운 다수 클래스를 남김.

---

## ✅ 과소표집의 단점
- 데이터를 줄이니까 **모델이 학습할 정보가 부족해질 수 있음**.
- 오버피팅 위험도 있음.

---

## ✅ 1. ENN (Edited Nearest Neighbors)

### 💡 개념:
- **K-NN 기반**으로 작동해.
- **자기 클래스와 다른 클래스에 둘러싸인 데이터(잡음)**를 제거해서 **결정 경계를 더 명확하게** 만드는 방식이야.

### 🧠 동작 방식:
1. 각 데이터 포인트에 대해 **가장 가까운 K개의 이웃**을 찾는다.
2. 이웃의 **다수 클래스와 다르면 해당 샘플을 제거**한다.
   - 즉, **자기 주변 이웃들과 다른 레이블을 가진 샘플**은 제거함.

### ✅ 특징:
- **노이즈 제거**, **경계 명확화**에 효과적.
- 다수 클래스뿐 아니라 **소수 클래스에서도 데이터가 제거될 수 있음** (주의!).

---

## ✅ 2. Tomek Links

### 💡 개념:
- **두 클래스 간 경계가 애매한 샘플 쌍을 제거**해서 데이터 분포를 개선함.
- 불필요한 중첩 샘플을 없애는 데 유용.

### 🧠 동작 방식:
1. 두 샘플 A와 B가 서로를 가장 가까운 이웃으로 가지고 있고, **클래스가 다르면** → **Tomek Link**라고 부름.
2. 이 경우, **대개 다수 클래스 샘플(B)을 제거**.

### ✅ 특징:
- 경계에 있는 **애매한 샘플을 줄여줌**.
- **모델의 분류 경계가 더 명확**해짐.

---

## ✅ 3. CNN (Condensed Nearest Neighbor)

### 💡 개념:
- KNN 알고리즘을 단순화해서, **결정 경계를 유지하는 최소한의 샘플만 남김**.

### 🧠 동작 방식:
1. 초기에는 소수 샘플 몇 개만 선택.
2. 이 샘플로 KNN을 돌렸을 때 **잘못 분류되는 샘플들을 점점 추가**.
3. 더 이상 잘못 분류되는 게 없을 때까지 반복.

### ✅ 특징:
- **결정 경계를 유지하는 데 필요한 데이터만 남김**.
- 훈련 데이터 양이 많이 줄어듦.
- 단점: **초기 샘플 선택에 따라 결과가 다를 수 있음**.

---

## ✅ 4. OSS (One-Sided Selection)

### 💡 개념:
- **Tomek Links + CNN**을 결합한 방식.

### 🧠 동작 방식:
1. Tomek Links로 **중첩된 경계 샘플 제거**.
2. CNN으로 **핵심 샘플만 남김**.

### ✅ 특징:
- **과소표집이지만 정보 손실을 최소화**하려는 전략.
- 효율적으로 다수 클래스를 줄일 수 있음.
- 다만 계산 비용은 조금 높은 편.

---

## 📝 요약 비교

| 기법 | 목적 | 방식 | 장점 | 단점 |
|------|------|------|------|------|
| ENN | 노이즈 제거 | KNN 기준 이웃과 다르면 제거 | 경계 명확화 | 소수 클래스도 손실 가능 |
| Tomek Links | 경계 단순화 | 다른 클래스와 가까운 쌍 제거 | 중복 제거, 경계 명확 | 단독으론 부족할 수 있음 |
| CNN | 데이터 압축 | 결정 경계 유지 최소 샘플 유지 | 데이터량 축소 | 초기 샘플 민감 |
| OSS | 정보 보존하면서 과소표집 | Tomek Links + CNN | 효율적, 정보 유지 | 계산량 많음 |
