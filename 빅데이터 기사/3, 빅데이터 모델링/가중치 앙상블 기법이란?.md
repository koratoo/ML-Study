가중치를 부여하는 앙상블 기법은 **각 개별 모델의 예측 결과에 가중치를 부여하여 최종 예측을 수행하는 방법**입니다. 일반적인 앙상블 학습 기법인 **배깅(Bagging)**, **부스팅(Boosting)**, **스태킹(Stacking)** 중에서도 특히 **부스팅과 스태킹**이 가중치를 활용하는 대표적인 방법입니다.

---

### 1. 가중치를 부여하는 주요 앙상블 기법
#### ✅ **부스팅(Boosting)**
- 이전 모델이 틀린 샘플에 가중치를 증가시켜서, 다음 모델이 더 잘 학습하도록 하는 방법.
- 대표 알고리즘: **AdaBoost, Gradient Boosting (GBM), XGBoost, LightGBM, CatBoost**
- 예제:
  1. 처음에는 모든 데이터에 동일한 가중치를 줌.
  2. 첫 번째 모델이 학습하고 예측을 수행.
  3. 틀린 샘플의 가중치를 증가시킴.
  4. 두 번째 모델이 업데이트된 가중치로 학습.
  5. 이런 과정을 반복하여 점점 더 정교한 모델을 만듦.
  6. 최종적으로 여러 모델의 예측을 가중 평균하여 최종 결괏값을 도출.

---

#### ✅ **스태킹(Stacking)**
- 여러 개의 서로 다른 모델을 결합하여 최적의 조합을 찾는 방식.
- 일반적으로 **메타 모델(Meta Learner, 블렌더)**를 사용하여 개별 모델의 예측값을 입력으로 받아 최종 결정을 내림.
- 예제:
  1. 여러 개의 기본 모델을 훈련.
  2. 각 모델의 예측값을 입력으로 사용하는 **메타 모델**을 훈련.
  3. 메타 모델은 개별 모델들의 예측값을 최적의 가중치를 이용해 조합하여 최종 예측을 수행.

---

### 2. 가중치를 부여하는 방식
가중치를 부여하는 방법은 크게 3가지가 있음.

#### ✅ **(1) 모델별 가중치 조합**
- 각각의 모델이 예측한 값을 가중 평균하여 최종 결과를 결정.
- 가중치 조정 방식:
  - **경험적 조정**: 성능이 좋은 모델에 더 높은 가중치 부여.
  - **학습 기반 조정**: 학습 데이터를 활용해 최적의 가중치를 자동으로 학습.

\[
y_{\text{final}} = w_1 \cdot y_1 + w_2 \cdot y_2 + w_3 \cdot y_3 + \dots
\]

- 예제 (Soft Voting):
  ```python
  from sklearn.ensemble import VotingClassifier
  from sklearn.linear_model import LogisticRegression
  from sklearn.ensemble import RandomForestClassifier
  from sklearn.svm import SVC

  clf1 = LogisticRegression()
  clf2 = RandomForestClassifier(n_estimators=100)
  clf3 = SVC(probability=True)

  ensemble = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)], 
                              voting='soft', weights=[0.2, 0.3, 0.5])
  ```

---

#### ✅ **(2) 데이터 샘플별 가중치 조정**
- **부스팅(Boosting) 기법에서 사용됨.**
- 이전 모델이 틀린 샘플의 가중치를 증가시켜 학습.

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

base_model = DecisionTreeClassifier(max_depth=1)
boosted_model = AdaBoostClassifier(base_model, n_estimators=50, learning_rate=0.1)
```

---

#### ✅ **(3) 메타 모델 기반 가중치 학습 (Stacking)**
- 여러 모델의 예측값을 모아 최적 가중치를 학습하는 메타 모델 활용.

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

base_models = [
    ('rf', RandomForestClassifier(n_estimators=100)),
    ('svc', SVC(probability=True))
]

meta_model = LogisticRegression()

stacking = StackingClassifier(estimators=base_models, final_estimator=meta_model)
```

---

### 3. 정리
- **부스팅(Boosting)**: 틀린 샘플에 가중치를 증가시키면서 모델을 학습.
- **스태킹(Stacking)**: 여러 모델의 예측값을 메타 모델이 학습하여 최적의 조합을 찾음.
- **가중 평균(Voting)**: 모델의 성능에 따라 가중치를 부여하여 최종 예측값을 결정.

가중치를 부여하는 앙상블 기법은 **단순 평균보다 성능을 높이는 데 효과적**이므로 실무에서도 많이 활용됨.
